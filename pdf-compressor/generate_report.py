#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á–µ—Ç–æ–≤ –æ —Å–∂–∞—Ç–∏–∏ PDF —Ñ–∞–π–ª–æ–≤
"""

import sys
import json
import re
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List


def parse_log_file(log_path: str) -> Dict[str, Any]:
    """–ü–∞—Ä—Å–∏–Ω–≥ –ª–æ–≥-—Ñ–∞–π–ª–∞ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
    
    log_file = Path(log_path)
    if not log_file.exists():
        return {'error': f'Log file not found: {log_path}'}
    
    try:
        with open(log_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        stats = {
            'processed_files': 0,
            'failed_files': 0,
            'total_size_before': 0,
            'total_size_after': 0,
            'total_bytes_saved': 0,
            'files': [],
            'errors': [],
            'duration': '0',
            'compression_level': 'unknown',
            'start_time': '',
            'end_time': ''
        }
        
        # –†–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
        patterns = {
            'file_start': r'üìÑ \[(\d+)/(\d+)\] (.+)',
            'file_size': r'üìä –†–∞–∑–º–µ—Ä: (.+)',
            'compression_result': r'üíæ –≠–∫–æ–Ω–æ–º–∏—è: (.+) \((.+)%\)',
            'error': r'‚ùå (.+)',
            'duration': r'‚è±Ô∏è –í—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã: (.+)',
            'processed': r'‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: (\d+)',
            'failed': r'‚ùå –û—à–∏–±–æ–∫: (\d+)',
            'total_before': r'üìä –†–∞–∑–º–µ—Ä –¥–æ —Å–∂–∞—Ç–∏—è: (.+)',
            'total_after': r'üìä –†–∞–∑–º–µ—Ä –ø–æ—Å–ª–µ —Å–∂–∞—Ç–∏—è: (.+)',
            'total_saved': r'üíæ –û–±—â–∞—è —ç–∫–æ–Ω–æ–º–∏—è: (.+)',
            'compression_level': r'üóúÔ∏è –£—Ä–æ–≤–µ–Ω—å —Å–∂–∞—Ç–∏—è: (\w+)',
            'start_time': r'‚è∞ –í—Ä–µ–º—è –∑–∞–ø—É—Å–∫–∞: (.+)',
        }
        
        lines = content.split('\n')
        current_file = None
        
        for line in lines:
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—É—é —Å—Ç—Ä–æ–∫—É
            for pattern_name, pattern in patterns.items():
                match = re.search(pattern, line)
                if match:
                    if pattern_name == 'file_start':
                        current_file = {
                            'name': match.group(3),
                            'index': int(match.group(1)),
                            'total': int(match.group(2))
                        }
                    elif pattern_name == 'file_size' and current_file:
                        current_file['original_size_str'] = match.group(1)
                    elif pattern_name == 'compression_result' and current_file:
                        current_file['savings'] = match.group(1)
                        current_file['percent_saved'] = float(match.group(2))
                        stats['files'].append(current_file)
                        current_file = None
                    elif pattern_name == 'error':
                        stats['errors'].append(match.group(1))
                    elif pattern_name == 'duration':
                        stats['duration'] = match.group(1)
                    elif pattern_name == 'processed':
                        stats['processed_files'] = int(match.group(1))
                    elif pattern_name == 'failed':
                        stats['failed_files'] = int(match.group(1))
                    elif pattern_name == 'total_before':
                        stats['total_size_before_str'] = match.group(1)
                    elif pattern_name == 'total_after':
                        stats['total_size_after_str'] = match.group(1)
                    elif pattern_name == 'total_saved':
                        stats['total_saved_str'] = match.group(1)
                    elif pattern_name == 'compression_level':
                        stats['compression_level'] = match.group(1)
                    elif pattern_name == 'start_time':
                        stats['start_time'] = match.group(1)
        
        return stats
        
    except Exception as e:
        return {'error': f'Error parsing log file: {str(e)}'}


def load_json_stats(stats_path: str = 'temp/logs/stats.json') -> Dict[str, Any]:
    """–ó–∞–≥—Ä—É–∑–∫–∞ JSON —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞"""
    
    stats_file = Path(stats_path)
    if not stats_file.exists():
        return {}
    
    try:
        with open(stats_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception:
        return {}


def format_file_size(bytes_size: int) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ —Ñ–∞–π–ª–∞"""
    if bytes_size == 0:
        return "0 B"
    
    for unit in ['B', 'KB', 'MB', 'GB']:
        if bytes_size < 1024.0:
            return f"{bytes_size:.1f} {unit}"
        bytes_size /= 1024.0
    return f"{bytes_size:.1f} TB"


def generate_report(log_path: str, stats_path: str = None) -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ –æ —Å–∂–∞—Ç–∏–∏"""
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –ª–æ–≥–∞
    log_stats = parse_log_file(log_path)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º JSON —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞
    json_stats = {}
    if stats_path:
        json_stats = load_json_stats(stats_path)
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç JSON)
    stats = {**log_stats, **json_stats}
    
    if 'error' in stats:
        return f"# ‚ùå Error generating report\n\n{stats['error']}"
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç
    report = []
    
    # –ó–∞–≥–æ–ª–æ–≤–æ–∫
    report.append("# üìä PDF Compression Report")
    report.append("")
    
    # –û–±—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S UTC')
    report.append(f"**Generated:** {timestamp}")
    report.append(f"**Compression Level:** {stats.get('compression_level', 'unknown')}")
    report.append(f"**Duration:** {stats.get('duration', 'unknown')}")
    report.append("")
    
    # –û—Å–Ω–æ–≤–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    report.append("## üìà Summary")
    report.append("")
    
    processed = stats.get('processed_files', 0)
    failed = stats.get('failed_files', 0)
    total = processed + failed
    
    if total > 0:
        success_rate = (processed / total) * 100
        report.append(f"- ‚úÖ **Successfully processed:** {processed} files ({success_rate:.1f}%)")
        
        if failed > 0:
            report.append(f"- ‚ùå **Failed:** {failed} files")
        
        # –≠–∫–æ–Ω–æ–º–∏—è –º–µ—Å—Ç–∞
        if 'total_bytes_saved' in stats and stats['total_bytes_saved'] > 0:
            total_saved = format_file_size(stats['total_bytes_saved'])
            percent_saved = stats.get('total_percent_saved', 0)
            
            report.append(f"- üíæ **Space saved:** {total_saved} ({percent_saved:.1f}%)")
            
            if 'total_size_before' in stats:
                size_before = format_file_size(stats['total_size_before'])
                size_after = format_file_size(stats.get('total_size_after', 0))
                report.append(f"  - Before: {size_before}")
                report.append(f"  - After: {size_after}")
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ –ª–æ–≥–∞ (–µ—Å–ª–∏ JSON –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω)
        elif 'total_saved_str' in stats:
            report.append(f"- üíæ **Space saved:** {stats['total_saved_str']}")
            if 'total_size_before_str' in stats:
                report.append(f"  - Before: {stats['total_size_before_str']}")
                report.append(f"  - After: {stats.get('total_size_after_str', 'unknown')}")
    else:
        report.append("- ‚ÑπÔ∏è **No files processed**")
    
    report.append("")
    
    # –î–µ—Ç–∞–ª–∏ –ø–æ —Ñ–∞–π–ª–∞–º
    files = stats.get('files', [])
    if files and len(files) <= 10:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –¥–µ—Ç–∞–ª–∏ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Ñ–∞–π–ª–æ–≤ –Ω–µ–º–Ω–æ–≥–æ
        report.append("## üìÑ Processed Files")
        report.append("")
        
        for file_info in files:
            if isinstance(file_info, dict):
                name = file_info.get('name', 'unknown')
                
                # –ò–∑ JSON —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
                if 'percent_saved' in file_info:
                    savings = f"{file_info['percent_saved']:.1f}%"
                    if 'bytes_saved' in file_info:
                        bytes_saved = format_file_size(file_info['bytes_saved'])
                        savings = f"{bytes_saved} ({savings})"
                # –ò–∑ –ª–æ–≥–∞
                elif 'savings' in file_info:
                    savings = f"{file_info['savings']} ({file_info.get('percent_saved', 0):.1f}%)"
                else:
                    savings = "unknown"
                
                report.append(f"- **{name}:** {savings}")
        
        report.append("")
    
    elif len(files) > 10:
        report.append(f"## üìÑ Files Summary")
        report.append("")
        report.append(f"Processed {len(files)} files. Top performers:")
        report.append("")
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø—Ä–æ—Ü–µ–Ω—Ç—É —ç–∫–æ–Ω–æ–º–∏–∏ –∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ø-5
        sorted_files = sorted(files, 
                            key=lambda x: x.get('percent_saved', 0), 
                            reverse=True)
        
        for file_info in sorted_files[:5]:
            if isinstance(file_info, dict):
                name = file_info.get('name', 'unknown')
                percent = file_info.get('percent_saved', 0)
                report.append(f"- **{name}:** {percent:.1f}% saved")
        
        report.append("")
    
    # –û—à–∏–±–∫–∏
    errors = stats.get('errors', [])
    if errors:
        report.append("## ‚ùå Errors")
        report.append("")
        
        for error in errors[:5]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5 –æ—à–∏–±–æ–∫
            if isinstance(error, dict):
                file_name = error.get('file', 'unknown')
                error_msg = error.get('error', 'unknown error')
                report.append(f"- **{file_name}:** {error_msg}")
            elif isinstance(error, str):
                report.append(f"- {error}")
        
        if len(errors) > 5:
            report.append(f"- ... and {len(errors) - 5} more errors")
        
        report.append("")
    
    # –°—Ç–∞—Ç—É—Å
    if processed > 0 and failed == 0:
        status_emoji = "‚úÖ"
        status_text = "All files processed successfully"
    elif processed > 0 and failed > 0:
        status_emoji = "‚ö†Ô∏è"
        status_text = f"Partially successful ({failed} failures)"
    elif failed > 0:
        status_emoji = "‚ùå"
        status_text = "Processing failed"
    else:
        status_emoji = "‚ÑπÔ∏è"
        status_text = "No files to process"
    
    report.append(f"## {status_emoji} Status")
    report.append("")
    report.append(f"**{status_text}**")
    report.append("")
    
    # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
    if json_stats:
        report.append("---")
        report.append("*Report generated from JSON statistics*")
    else:
        report.append("---")
        report.append("*Report generated from log file parsing*")
    
    return "\n".join(report)


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    if len(sys.argv) < 2:
        print("Usage: python generate_report.py <log_file> [stats_file]")
        sys.exit(1)
    
    log_file = sys.argv[1]
    stats_file = sys.argv[2] if len(sys.argv) > 2 else None
    
    report = generate_report(log_file, stats_file)
    print(report)


if __name__ == "__main__":
    main()
